<img src="Images/ComponentMethods.PNG" width="600">

# Dimensionality Reduction
- Discover hidden correlations/topics
- Remove redundant & noisy features
- Interpretation & Visualization
- Easier storage & processing of data


# Principal Component Analysis
<img src="Images/PCA_BigPicture.PNG" width="500">

> - Linear Combination of the original variables
> - Identify directions "Principal Components" along which the variation in the data is maximal
> - "Find a lower dimensional line/plane on which to project the data so that the sum of square is min. OR Find the line that maximizes the distances from the projected points to the origin"

**Eigenvalue**: The amount of variance retained by each Principal Component

## Algorithm
"Reduce from n-dimensions to k-dimensions: Find k vectors [u1,u2,...,uk] onto which to project the data so as to min the projection error"

# Correspondence Analysis
>  extension of the principal component analysis for analyzing a large contingency table formed by two qualitative
variables (or categorical data)

# Multiple Correspondence Analysis
> adaptation of CA to a data table containing more than two categorical variables.

# Factor Analysis of Mixed Data
> dedicated to analyze a data set containing both quantitative and qualitative variables.

# Multiple Factor Analysis
> dedicated to analyze data sets, in which variables are organized into groups (qualitative and/or quantitative variables)
